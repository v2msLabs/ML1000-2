---
title: Gun Violence in the US. Application of Unsupervised Learning Methods for Trend Exploration
author:
  - name: Sumaira Afzal
    affiliation: York University School of Continuing Studies
  - name: Viraja Ketkar
    affiliation: York University School of Continuing Studies
  - name: Murlidhar Loka
    affiliation: York University School of Continuing Studies
  - name: Vadim Spirkov
    affiliation: York University School of Continuing Studies
abstract: >
  Gun deaths in US rise to highest level in 20 years. Forty thousand people were killed in shootings in 2017 amid a growing number of suicides involving firearms. Research by the Educational Fund to Stop Gun Violence underlines that the tragedy of gun violence and suicides is not spread randomly across the country, but is concentrated precisely in those places where gun ownership is most prevalent and gun laws at their loosest. Using the data collected by Gun Violence Archive (GVA) and employing unsupervised learning methods we will make an attempt to provide additional insights into the nature of this sad  statistics. 

output:
  rticles::rjournal_article:
    includes:
      in_header: preamble.tex
   
---

```{r echo=FALSE, message=FALSE, warnings=FALSE}
# Clean all variables that might be left by other script to avoid collusion
rm(list=ls(all=TRUE))
# load required libraries
library(ggplot2) # plotting lib
library(gridExtra) # arrange grids
library(dplyr)  # data manipuation
library(RColorBrewer) # color palettes
library(tm) # text mining
library(dbscan) # density-based clustering
library(wordcloud) # plots fequent terms
library(factoextra) # deals with cluster plotting. Provides cluster related utility methods 
library(cluster) # for gower similarity and pam
library(plot3D) # 3D plots
library(summarytools)
source('utils.R') # supplementary code

# set summarytools global parameters
st_options(plain.ascii = F,       # This is very handy in all Rmd documents
      style = "rmarkdown",        # This too
      footnote = NA,             # Avoids footnotes which would clutter the result
      subtitle.emphasis = F,  # This is a setting to experiment with - according to
      dfSummary.graph.col = F
)  

# pick a palette
mainPalette = ggplotColours()
```

```{r global_options, include=FALSE}
# make the images flow nicely
knitr::opts_chunk$set(fig.pos = 'H', echo = T,comment = NA, prompt = F, 
                      cache = F, warning = F, message = F,  fig.align="center")
```


## Background

Gun violence in the US is a going problem. Mass-shooting, gun-assited suisides, accidental use of guns that cause death have become regular topic of the news agencies.  


## Objective

The objective of this research is to discover trends and tendencies of the gun violence situation in the US employing unsupervised learning algorithms.


# Data Analysis

The data set used for this research contains 260k of gun violence incidents in the US between January 2013 and March 2018. The data has been sourced from [Kaggle](https://www.kaggle.com/jameslko/gun-violence-data). 

Originally the data set was uploaded to Kaggle from Gun Violence Archive (GVA)  Web site [ gunviolencearchive.org](https://www.gunviolencearchive.org/). This is a not for profit corporation formed in 2013 to provide free online public access to accurate information about gun-related violence in the United States. GVA will collect and check for accuracy, comprehensive information about gun-related violence in the U.S. and then post and disseminate it online. 


## Data Dictionary


Column Name                 | Column Description  
----------------------------| ------------------- 
incident_id                 | Incident ID
date                        | Date of crime
state                       | State
city_or_county              | City/county of crime
address                     | Address of the location of the crime
n_killed                    | Number of people killed
n_injured                   | Number of people injured
incident_url                | URL regarding the incident
source_url                  | Reference to the reporting source
incident_url_fields_missing | TRUE if the incident_url is present, FALSE otherwise
congressional_district      | Congressional district id
gun_stolen                  | Status of guns involved in the crime (i.e. Unknown, Stolen, etc...)
gun_type                    | Typification of guns used in the crime
incident_characteristics    | Characteristics of the incidence
latitude                    | Location of the incident
location_description        | Description of the location
longitude                   | Location of the incident
n_guns_involved             | Number of guns involved in incident
notes                       | Additional information of the crime
participant_age             | Age of participant(s) at the time of crime (victims and suspects)
participant_age_group       | Age group of participant(s) at the time crime
participant_gender          | Gender of participant(s)
participant_name            | Name of participant(s) involved in crime
participant_relationship    | Relationship of participant to other participant(s)
participant_status          | Extent of harm done to the participant
participant_type            | Type of participant (victim or suspect)
sources                     | Participants source
state_house_district        | Voting house district
state_senate_district       | Territorial district from which a senator to a state legislature is elected.


## Data Exploration

Firstly we are going to load and examine content and statistics of the data set

```{r}
data = read.csv("../data/gun-violence-data_01-2013_03-2018.csv", header = T, 
                na.strings = c("NA","","#NA"),sep=",")
```

```{r dataset_summary, echo=FALSE, results="asis"}
print(dfSummary(data, valid.col = F, max.distinct.values = 3, heading = F),
       caption = "\\label{tab:dataset_summary} Gun Violence Dataset Summary", scalebbox = .9)
```

Initial observation of the data shows that there is a number of features which do not present
any analytical value (Table:  \ref{tab:dataset_summary}). They are:

* *incident_id*
* *incident_url*
* *source_url* 
* *state_house_district*
* *state_senate_district* 
* *congressional_district*
* *sources*
* *incident_url_fields_missing*

We are going to get rid of them. We will also drop *participant_age* feature in favor of the *participant_age_group*. The age group is more suitable for categorization and has much less missing data (16% vs 39%). The remaining features could be grouped as follows...

##### Participant Features.

This group describes suspects and victims found on the crime scene. The content of the features of this group is structured as follows: *[idx1::value1||idx2::value2]* (see Table: \ref{tab:dataset_summary}). This is not quite acceptable for the analytic, thus the participant related features would have to be parsed to extract valuable information about the crime.

It is feasible. *utils.R* script contains *parseFeature* function, which parses *[idx1::value1||idx2::value2]* structure and returns a named vector object. For example a  *participent_type* could be structured as follows:

0            | 1            | 2               | 3
-------------| -------------| ----------------| ---------------  
Victim       | Victim       | Subject-Suspect | Subject-Suspect

Unfortunately *participant_relationship* feature missing **93%** of values. It is not possible to impute the missing data thus we will drop it. For obvious reasons we are also going to get rid of *participant_name*. The rest of the participant-related features will be parsed and replaced wit the new categorical attributes. In order to do so we have to understand what possible values each participant-related feature can have. To do so  we will employ text mining technique (Ref: \cite{tm}). We will analyze term frequencies to make conclusions about the content of the features. 

We begin with *participant_type* feature

```{r}
participantType = data %>%  mutate(text = trimws(gsub('\\|\\||:|\\|'," ",participant_type, 
        fixed = F))) %>% filter(text != "0" ) %>% select(text)

pCorups = VCorpus(VectorSource(participantType))
pCorups  = tm_map(pCorups , removeNumbers)
pTermMatrix = tm::TermDocumentMatrix(pCorups)
print(tm::findFreqTerms(pTermMatrix, 10))
```
As we can see the *participan type* may have two values **vitim** and **subject-suspect**. If the *participant type* is missing we will consider it as **unknown**. Thus we will be employing *participant_type* feature as a basis to impute all other participant related stats.

Let's find the possible values of *parctipant_age_group* feature (the coded is omitted).
```{r echo=FALSE}
participantAgeGroup = data %>%  mutate(text = trimws(gsub('\\|\\||:|\\|'," ",participant_age_group,
      fixed = F))) %>% filter(text != "0" ) %>% select(text)
participantAgeGroup = distinct(participantAgeGroup)
pCorups = VCorpus(VectorSource(participantAgeGroup))
pCorups  = tm_map(pCorups , removeNumbers)
pTermMatrix = tm::TermDocumentMatrix(pCorups)
print(tm::findFreqTerms(pTermMatrix, 10))
```
Further examination of the feature data shows that there the age group values are:

* Adult 18+
* Teen 12-17
* Child 0-11

Thus using *participant_age_group* feature data we will create two new ones: *vicitm_age_group* and *suspect_age_group*. These new categorical features will be coded as follows: 

* 0 - no info
* 1 - all adults
* 2 - children/ teens
* 3 - adults and children/ teens . Adults make majority
* 4 - adults and children/ teens. Children/ teens make majority

*participant_gender* could also be parsed and replaced with the coded categorical features as described below.
```{r}
participantGender = data %>%  mutate(text = trimws(gsub('\\|\\||:|\\|'," ",participant_gender, 
        fixed = F))) %>% filter(text != "0" ) %>% select(text)
pCorups = VCorpus(VectorSource(participantGender))
pCorups  = tm_map(pCorups , removeNumbers)
pTermMatrix = tm::TermDocumentMatrix(pCorups)
print(tm::findFreqTerms(pTermMatrix, 10))
```
As a result we will be adding two new features:

* *victim_gender* - gender of the victims
* *suspect_gender* - gender of the suspects

**Gender Codes**

* 0 - no info
* 1 - male
* 2 - female
* 3 - male dominated group
* 4 - female dominated group

The last feature of the group is *participant_status*. It maintains the outcome of the incident. Let's review the content of the attribute.

```{r echo=FALSE}
participantStatus = data %>%  mutate(text = trimws(gsub('\\|\\||:|\\|'," ",participant_status, 
        fixed = F))) %>% filter(text != "0" ) %>% select(text)
pCorups = VCorpus(VectorSource(participantStatus))
pCorups  = tm_map(pCorups , removeNumbers)
pTermMatrix = tm::TermDocumentMatrix(pCorups)
print(tm::findFreqTerms(pTermMatrix, 10))
```
Based on our findings we will be creating three new numerical features:

* n_victim_killed - number of victims killed
* n_victim_injured - number of victims injured
* n_arrested - number of suspects arrested

##### Gun Related Features. 

There are three attributes that describe gun types: *gun_stolen*, *gun_type* and *n_guns_invoved* (Table: \ref{tab:dataset_summary}) *gun_type* and *gun_stolen* have similar to the participant-related features encoding (*[idx1::value1||idx2::value2]*). Thus they also could be parsed and substituted with the categorical features.
We begin with the gun type.
```{r plot_gun_types,  echo=FALSE, fig.align="center", fig.cap="The Most Frequently Used Gun Types"}
gunTypes = data %>%  mutate(text = trimws(gsub('\\|\\||:|\\||Unknown'," ",gun_type, fixed = F))) %>% 
  filter(text != "0" ) %>% select(text)
pCorups = VCorpus(VectorSource(gunTypes))
pCorups  = tm_map(pCorups , removeNumbers)
pCorups  = tm_map(pCorups , removePunctuation)
pTermMatrix = tm::TermDocumentMatrix(pCorups)
tmp = as.matrix(pTermMatrix)
tmp = sort(rowSums(tmp),decreasing=T)
tmp = data.frame(word = names(tmp),freq=tmp)
ggplot(tmp,aes(x=word, y=freq )) + 
      geom_col(fill=mainPalette[8], colour=mainPalette[8], alpha = 0.5)
```
Employing simple text mining techniques we can see that **handgun**, **rifle**, **shotgun** and **auto** make the majority. Thus we will add another new feature *gun_type_involved* to categorize the gun types as follows:

* 0 - unknown 
* 1 - handgun
* 2 - shotgun/ rifle
* 3 - automatic
* 4 - mix/other 

*gun_stolen* attribute tells if the gun was stolen or acquired legally. We are going to create a new categorical feature - *gun_origin* which would maintain the following data:

* 0 - unknown
* 1 - all stolen
* 2 - all acquired legally
* 3 - mix of stolen and legal guns


##### Location Related Features.

To analyze geography of the crimes we will be employing *state*, *city_or_county*, *latitude* and *longitude* attribute. Since we have the coordinates the *address* feature does not present much value for unsupervised learning. We will be using it though to impute missing latitude and longitude values. This activity will be covered in greater details in **Missing Data** paragraph.

#### Descriptive Features.

*notes*, *location_description* and *incident_characteristics* are free-text features that might provide additional insights about the crime scene. We are going to take a close look at each feature and decide if we could utilize it.

Lets' begin with the *notes*
```{r plot_notes,  echo=FALSE, fig.align="center", fig.cap="Most Common Words in Notes"}
notes = data %>%  mutate(text = trimws(notes)) %>% filter(!is.na(text)) %>% select(text)
pCorups = VCorpus(VectorSource(notes))
pCorups  = tm_map(pCorups , removeNumbers)
pCorups  = tm_map(pCorups , removePunctuation)
pTermMatrix = tm::TermDocumentMatrix(pCorups)
tmp = as.matrix(pTermMatrix)
tmp = sort(rowSums(tmp),decreasing=T)
tmp = data.frame(word = names(tmp),freq=tmp)
set.seed(5673)
wordcloud(words = tmp$word, freq = tmp$freq, min.freq = 10,max.words=200, random.order=F,
          rot.per=0.35, colors=mainPalette)
```
Unfortunately *notes* feature does not provide more knowledge to what the others features already supply. Thus it will be dropped.

*location_description* on the other hand, could be useful to classify location type. Unfortunately 82% of the data is missing. Nonetheless this feature appears to be too important to ignore. Let's see how much we can salvage.
```{r plot_location,  echo=FALSE, fig.align="center", fig.cap="Most Common Bi-gram Terms in Location Decription"}
location = data %>%  mutate(text = trimws(location_description)) %>% filter(!is.na(text)) %>% select(text)
pCorups = VCorpus(VectorSource(location))
pCorups  = tm_map(pCorups , removeNumbers)
pCorups  = tm_map(pCorups , removePunctuation)
pCorups = tm_map(pCorups, removeWords, stopwords())
pTermMatrix = tm::TermDocumentMatrix(pCorups, control = list(tokenize = BigramTokenizer))
tmp = as.matrix(pTermMatrix)
tmp = sort(rowSums(tmp),decreasing=T)
tmp = data.frame(word = names(tmp),freq=tmp)
set.seed(901)
wordcloud(words = tmp$word, freq = tmp$freq,min.freq=30, max.words=100, random.order=F,
          rot.per=0.3, colors=mainPalette,scale=c(3,.5))
```

As plot \ref{fig:plot_location} shows we could utilize bi-gram terms if the location description is available. Thus let's add new categorical feature - *place_type*, which would have the following values:

* 0 - unknown
* 1 - school/ university/ college
* 2 - community center/ shopping center/ hospital/ church
* 3 - home invasion
* 4 - street/drive by
* 5 - other public places

The last feature in the group is *incident_characteristics*. The feature is almost 100% populated. As with the previous two we are going to find out what info it maintains.

```{r plot_freq_char, echo=FALSE, fig.align="center", fig.cap="Most Common Bi-gram Terms in Incident Characteristics Column" }
characteristics = data %>%  mutate(text = gsub('\\|\\||/'," ",incident_characteristics, fixed = F))  %>%                 filter(!is.na(text)) %>% select(text)
pCorups = VCorpus(VectorSource(characteristics))
pCorups  = tm_map(pCorups , removeNumbers)
pCorups  = tm_map(pCorups , removePunctuation)
pCorups = tm_map(pCorups, removeWords, stopwords())
pTermMatrix = tm::TermDocumentMatrix(pCorups, control = list(tokenize = BigramTokenizer))
tmp = as.matrix(pTermMatrix)
tmp = sort(rowSums(tmp),decreasing=T)
tmp = data.frame(word = names(tmp),freq=tmp) 
tmp = tmp %>% top_n(n = 50)
ggplot(tmp, aes(x=word, y=freq )) + 
      geom_col(fill=mainPalette[1], colour=mainPalette[1], alpha = 0.5,
               position = position_stack(reverse = T)) + coord_flip()
```

Information the *incident_characteristics* provides proved to be useful. It can support two features: *place_type*, which was introduced above and *inceident_type*. The *incident_type* is going to be a categorical attribute with the following codes:

* 0 - unknown
* 1 - accidental
* 2 - defensive use
* 3 - armed robbery
* 4 - suicide
* 5 - raid/ arrest/ warrant
* 6 - domestic violence
* 7 - gun brandishing, flourishing, open demonstration 
_
We are also be adding a feature that indicates if drugs or alcohol was involved: *is_drug_alcohol*

##### Date Feature

In addition to *date* of incident attribute we add *month*  and *day_of_week* to identify any seasonal patterns. 

## Data Preparation

Prior to generating new features as discussed in the previous paragraph we would need to impute missing latitude and longitude data. To do so we employ [OpenCage](https://opencagedata.com/) forward geocoding API. Unlike Google this company offers a free tier. To save time we imputed the missing geo-coordinates and saved the result in the file. The code below is submitted for demonstration purpose only.
```{r, eval = FALSE, echo = TRUE}
  imputeCoordinates()
```

Now we are going to remove the features identified as redundant
```{r}
data = subset(data, select = c(-incident_id, -incident_url, -source_url, 
  -state_house_district, -state_senate_district,-sources, -incident_url_fields_missing,
  -congressional_district, -address, -participant_age, -participant_name,
  -participant_relationship, -notes))
```

Lastly we are going to loop through the entire data fame imputing missing data and adding new features. Again this is a lengthy process that takes about 1.5 hours to finish. The code is also quite long. Thus in order not to clutter the report we submit the code just in the script to illustrates the process, but will not output it into the report.

```{r eval=FALSE, include=FALSE}
for (idx in 1:nrow(data)) {
 date = data[idx,"date"]
 data[idx,"month"] = as.numeric( format(date, "%m")) 
 data[idx,"day_of_week"] = as.numeric( format(date, "%u")) 
 # parse participant gender  
 participantGender = parseFeature(data[idx,"participant_gender"])  
 # parse participant type
 participantType = parseFeature(data[idx,"participant_type"])  
 # parse participant age group
 participantAgeGroup = parseFeature(data[idx,"participant_age_group"])  
 # parse participant status
 participantStatus = parseFeature(data[idx,"participant_status"])  
 # parse gun origin
 gunOrigin = data[idx,"gun_stolen"]  
 # parse gun type
 gunType = data[idx,"gun_type"]  
 # assign other features of interest to variables to optimize performance and simplify the reference
 numKilled = data[idx,"n_killed"]
 numInjured = data[idx,"n_injured"]
 numGuns = data[idx,"n_guns_involved"]
 characteristics = data[idx,"incident_characteristics"]
 locationDecription  = data[idx,"location_description"]

 # default new, participant-related  features
 data[idx,"victim_gender"]  = 0
 data[idx,"suspect_gender"]  = 0 
 data[idx,"victim_age_group"]  = 0 
 data[idx,"suspect_age_group"]  = 0 
 data[idx,"n_victim_killed"]  = 0 
 data[idx,"n_victim_injured"]  = 0 
 data[idx,"n_victims"]  = 0 
 data[idx,"n_suspects"]  = 0
 data[idx,"n_arrested"] = 0
 # process participant-related data
 if (!all(is.na(participantType))) {
   victims = participantType[which(participantType %in% c("Victim"))]
   suspects = participantType[which(participantType %in% c("Subject-Suspect"))]
   
   n_victims = length(victims)
   n_suspects = length(suspects)
   data[idx,"n_victims"] = n_victims
   data[idx,"n_suspects"] = n_suspects

   if (n_victims > 0) {
     
     injured = 0
     killed = 0
     female = 0
     male = 0
     child = 0
     teen = 0
     adult = 0
     
     for (i in names(victims)){
       ## outcome
       status = participantStatus[i]
       if ( !isNaOrNull(status) ) {
         if (str_count(status,fixed("injured", ignore_case = T)) > 0) {
           injured = injured + 1
         } else if (str_count(status,fixed("killed", ignore_case = T)) > 0) {
           killed = killed + 1
         } 
       }   
       # gender
       gen = participantGender[i]
       if (!isNaOrNull(gen)) {
         if (str_count(gen,regex("^male", ignore_case = T)) > 0) {
           male = male + 1
         } else if (str_count(gen,regex("^female", ignore_case = T)) > 0) {
           female = female + 1
         }
       }
       # age group
       age = participantAgeGroup[i]
       if (!isNaOrNull(age)) {
         if (str_count(age,fixed("teen", ignore_case = T)) > 0) {
           teen = teen + 1
         } else if (str_count(age,fixed("adult", ignore_case = T)) > 0) {
           adult = adult + 1
         } else if (str_count(age,fixed("child", ignore_case = T)) > 0) {
           child = child + 1
         } 
       }
     }
     
     # summarize
     data[idx,"n_victim_killed"] = killed
     data[idx,"n_victim_injured"] = injured
     if ((child > 0|| teen > 0) && adult  == 0) {
       data[idx,"victim_age_group"] = 2
     } else if ( (child > 0|| teen > 0) && adult >0){
       data[idx,"victim_age_group"] = ifelse(child + teen >= adult, 4, 3)
     } else if (adult > 0) {
       data[idx,"victim_age_group"] = 1   
     }
     
     
     if (female > 0 && male > 0) {
       data[idx,"victim_gender"] = ifelse(female > male, 4, 3)
     } else if (male > 0) {
       data[idx,"victim_gender"] = 1
     } else if (female > 0) {
       data[idx,"victim_gender"] = 2
     }
     
   }
 
   if (n_suspects >0) {
     arrested = 0
     female = 0
     male = 0
     child = 0
     teen = 0
     adult = 0  
     for (i in names(suspects)){
       ## outcome
       status = participantStatus[i]
       if ( !isNaOrNull(status) && str_count(status,fixed("arrested", ignore_case = T)) > 0) {
         arrested = arrested + 1
       }  
       # gender
       gen = participantGender[i]
       if (!isNaOrNull(gen)) {
         if (str_count(gen,regex("^male", ignore_case = T)) > 0) {
           male = male + 1
         } else if (str_count(gen,regex("^female", ignore_case = T)) > 0) {
           female = female + 1
         }
       }
       # age group
       age = participantAgeGroup[i]
       if (!isNaOrNull(age)) {
         if (str_count(age,fixed("teen", ignore_case = T)) > 0) {
           teen = teen + 1
         } else if (str_count(age,fixed("adult", ignore_case = T)) > 0) {
           adult = adult + 1
         } else if (str_count(age,fixed("child", ignore_case = T)) > 0) {
           child = child + 1
         } 
       }
     }
     
     # summarize
     data[idx,"n_arrested"] = arrested
     
     if ((child > 0|| teen > 0) && adult  == 0) {
       data[idx,"suspect_age_group"] = 2
     } else if ( (child > 0|| teen > 0) && adult >0){
       data[idx,"suspect_age_group"] = ifelse(child + teen >= adult, 4, 3)
     } else if (adult > 0) {
       data[idx,"suspect_age_group"] = 1   
     }
     
     if (female > 0 && male >0) {
       data[idx,"suspect_gender"] = ifelse(female > male, 4, 3)
     } else if (male > 0) {
       data[idx,"suspect_gender"] = 1
     } else if (female > 0) {
       data[idx,"suspect_gender"] = 2
     }   
   }
 }
 
 if ( isNaOrNull(numKilled) ) {
   data[idx,"n_killed"] = ifelse(data[idx,"n_victim_killed"] > 0,data[idx,"n_victim_killed"],0 )
 }
 
 if ( isNaOrNull(numInjured) ) {
   data[idx,"n_injured"] = ifelse(data[idx,"n_victim_injured"] > 0,data[idx,"n_victim_injured"],0 )
 }
 
 
 # process gun-related data
 data[idx,"gun_type_involved"]  = 0 
 data[idx,"gun_origin"]  = 0

  # gun origin
 if ( !isNaOrNull(gunOrigin) ) {
   stolen = str_count(gunOrigin,fixed("stolen", ignore_case = T))
   notStolen = str_count(gunOrigin,fixed("notstolen", ignore_case = T))
   if (stolen > 0 && notStolen > 0) {
     data[idx,"gun_origin"] = 3
   } else if (stolen > 0){
     data[idx,"gun_origin"] = 1
   } else if (notStolen > 0){
     data[idx,"gun_origin"] = 2
   }
 }

 shotgun = 0
 handgun = 0
 auto = 0
 other = 0
 if ( !isNaOrNull(gunType) ) { 
   shotgun = str_count(gunType,regex("shotgun|rifle", ignore_case = T))
   auto = str_count(gunType,regex("auto|ak-", ignore_case = T))
   handgun = str_count(gunType,regex("handgun|spl", ignore_case = T))
   other = str_count(gunType,regex("gauge|mag|other|rem|spr|win", ignore_case = T)) 
   
   if (other == 0 && shotgun == 0 && auto == 0 && handgun > 0 ) {
     data[idx,"gun_type_involved"]  = 1
   } else if (shotgun > 0 && auto == 0 && handgun == 0 && other == 0) {
     data[idx,"gun_type_involved"]  = 2
   } else if (shotgun == 0 && auto > 0 && handgun == 0 && other == 0) {
     data[idx,"gun_type_involved"]  = 3
   } else if (shotgun > 0 || auto > 0 || handgun> 0 || other > 0) {
     data[idx,"gun_type_involved"]  = 4
   } 
   
 }
 
 if ( isNaOrNull(numGuns) ) {
   data[idx,"n_guns_involved"] = shotgun + handgun + auto + other
 }
 
 # process location and characteristics 
 data[idx,"place_type"]  = 0 
 data[idx,"incident_type"]  = 0
 data[idx,"is_drug_alcohol"] = 0

 if ( !isNaOrNull(locationDecription) ) { 
   if (str_count( locationDecription,regex("school|university|college", ignore_case = T)) > 0){
     data[idx,"place_type"]  = 1
   } else if (str_count( locationDecription,regex("music hall|community|dollar|convenience store|circle k|church|food mart|medical center|shopping center|mall|hospital", ignore_case = T)) > 0) {
     data[idx,"place_type"]  = 2
   } else  if (str_count( locationDecription,regex("apartment|back yards|apartments|mobile home", ignore_case = T)) > 0) {
     data[idx,"place_type"]  = 3
   } else  if (str_count( locationDecription,regex("car", ignore_case = T)) > 0) {
     data[idx,"place_type"]  = 4
   }else  if (str_count( locationDecription,regex("village|neighborhood|bar|park|gas station|grill|east garfield|west garfield|waffle house|jail|taco bell|police department|bank|club|restaurant|pizza|inn|hotel|office", ignore_case = T)) > 0) {
     data[idx,"place_type"]  = 5
   }
 }

 if ( !isNaOrNull(characteristics) ) {
   if (str_count( characteristics,regex("accidental shooting|accidental suicide|murder accidental|injury accidental|accidental negligent|negligent discharge", ignore_case = T)) > 0){
     data[idx,"incident_type"]  = 1
   } else if (str_count( characteristics,regex("defensive use|use defensive|dgu", ignore_case = T)) > 0) {
     data[idx,"incident_type"]  = 2
   } else if (str_count( characteristics,regex("armed robbery|robbery with", ignore_case = T)) > 0) {
     data[idx,"incident_type"]  = 3
   } else if (str_count( characteristics,regex("suicide shot|suicide nonshooting|suicide officer", ignore_case = T)) > 0) {
     data[idx,"incident_type"]  = 4
   } else if (str_count( characteristics,regex("arrest|raid|warrant", ignore_case = T)) > 0) {
     data[idx,"incident_type"]  = 5
   } else if (str_count( characteristics,regex("domestic violence", ignore_case = T)) > 0) {
     data[idx,"incident_type"]  = 6
   } else if (str_count( characteristics,regex("brandishing|flourishing|open carry", ignore_case = T)) > 0) {
     data[idx,"incident_type"]  = 7
   }
   # impute place_type if required and possible
   if (data[idx,"place_type"] == 0) {
     if (str_count( characteristics,regex("home invasion", ignore_case = T)) > 0) {
       data[idx,"place_type"]  = 3
     } else if (str_count( characteristics,regex("car to car|car to street|drive-by|car shot", ignore_case = T)) > 0) {
       data[idx,"place_type"]  = 4
     } else if (str_count( characteristics,regex("business|institution group|club|bar", ignore_case = T)) > 0) {
       data[idx,"place_type"]  = 5
     } 
   }
   # if drugs/alcohol
   data[idx,"is_drug_alcohol"] = ifelse(str_count( characteristics,regex("drug|atf|alcohol|under influence",
                                  ignore_case = T)) > 0, 1, 0)   
     
 }   

}  
```

After we added new feature it is time to remove the columns that are no longer relevant and save the result into a file to be used for the unsupervised learning.

```{r}
data = subset(data, select = c(-participant_age_group, -participant_type,
      -participant_gender, -participant_status, -location_description, 
      -incident_characteristics, -gun_stolen,-gun_type))
```
```{r include=FALSE}
rm(data, gunTypes,location,notes, participantAgeGroup, participantGender,participantStatus,
   participantType, pCorups, pTermMatrix, tmp, characteristics)
```


## Resulting Dataset

After a rather lengthy process, we finally have reached the stage when our dataset is ready to be used for exploration by clustering algorithms. All missing features have been imputed and free-format text columns have been replaced with the categorical attributes.  This is the summary of the resulting data.


```{r include=FALSE}
data = read.csv("../data/gun-violence-engineered.csv", header = T,sep=",")
```
```{r dataset_engineered, results = 'asis'}
 print(dfSummary(data, valid.col = F, max.distinct.values = 3, headings = F),
       caption = "\\tt Engineered Gun Violence Dataset Summary", scalebox = .9)
```


# Modeling and Evalutation

In this section we will apply various clustering methods to explore gun violence trends in the US. We will use parallel, hierarchical and density-based clustering approaches. 

In general the clustering algorithms take time to compute the result. Thus we will be employing a smaller dataset to find and visualize the clusters. The focus of our interests will be the incidents, that have at least three victims and where a place type was recorded.

```{r}
victimStats = data %>% filter(n_victims >= 3 & place_type>0) %>% arrange(date)
```

Before we apply clustering models to the dataset we should assess clustering tendency. In order to do so we will employ **Hopkins** statistics.

## Hopkins Statistics

Hopkins statistic is used to assess the clustering tendency of a dataset by measuring the probability that a given dataset is generated by a uniform data distribution. Let's calculate Hopkins (**H**) statistics for some continuous variables:

* n_guns_involved
* n_victims
* n_suspects
* n_killed
* n_injured

The **H** value close to zero indicates very good clustering tendency. The **H** value around or greater than 0.5 denotes poor clustering tendency(Ref: \cite{factoextra}). 

```{r}
stats = victimStats[c("n_victims","n_suspects","n_guns_involved","n_killed","n_injured")] %>% scale()
H =  get_clust_tendency(stats,n = 100, graph = F, seed = 6709)
print(H[["hopkins_stat"]])
```

Outstanding!  **H** value is very close to 0. Let's calculate Hopkins statistics on some categorical features

```{r}
stats = victimStats[c("gun_type_involved","victim_gender","place_type","victim_age_group","suspect_gender")] %>% scale()
H =  get_clust_tendency(stats, n = 100, graph = F, seed = 6701)
print(H[["hopkins_stat"]])
```

Well, the categorical data do not seem to be so suitable for clustering, **H** greater than 0.3 is too high. Let's examine the combination of geo-coordinates and some continuous and categorical features.

```{r}
stats = victimStats[c("n_victims","n_suspects","n_guns_involved","n_killed","n_injured",
        "gun_type_involved","victim_gender","place_type","victim_age_group",
        "suspect_gender")] %>% scale()
H =  get_clust_tendency(stats, n = 100, graph = F, seed = 6701)
print(H[["hopkins_stat"]])
```
The **H** number is very encouraging again. It appears we would have to combine continuous with one or too categorical features to get dense, well separated clusters. Now it is time to explore the data

## Partitioning Clustering Approach Using CLARA

We decided to select CLARA method because it scales well and can deal with continuous and categorical data. It is based on The **Partitioning Around Medoids** (*PAM*) algorithm, which is a popular realization of k-medoids clustering. We start with univariate data set for simplicity. Then we will take a look at silhouette plot (Ref: \cite{factoextra}) to analyze the result. The silhouette coefficient measure how well the clusters are separated. The silhouette analysis also provides insights into the cluster density.

Let's see how *n_victim* feature could be split by *CLARA*. The method also scales the data.

```{r plot_clara1, echo = FALSE,  fig.cap="CLARA Approach. Univariate Feature, 4 Cluster Silhouette", out.width = "70%"}
stats = victimStats[c("n_victims")] 
clara = clara(stats, k = 4, metric = "euclidean",stand = T, samples = 100, sampsize =500)
fviz_silhouette(clara, title = "") +
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())
```
The result looks very good. The clusters are dense and well separated. The last cluster doe not look perfect, probably it contains the incidents, where number of victims is very high.
To better interpret the cluster content let's merge the clustering result with the original data set and render a scatter plot. 
```{r plot_clara2, echo = FALSE, fig.cap="CLARA Approach. Number of Victims/Cluster Scatter Plot", echo = FALSE, out.width = "70%"}
combined = cbind(stats, cluster=c(clara$clustering))
ggplot(data = combined, aes(x=cluster, y=n_victims)) +
   geom_point(size=3, color=mainPalette[1])
```

As we can see the cluster number one groups the incidents where number of victims is around 3. The cluster number 2 contains the incidents with 4 victims, the # 3 combines the incidents with the number of victims between 5 and 6. And the rest goes to the cluster #4. Here is the geo-location of the clustered incidents
```{r plot_clara3, fig.cap="CLARA Approach. Number of Victims Clusters on Map", echo = FALSE, out.width = "70%"}
combined = cbind(victimStats, cluster=c(clara$clustering))
ggplot(victimStats, aes(x=longitude, y=latitude )) + 
  geom_point( color= clara$clustering+1L)
```

Very well! Now we are going to add a categorical feature and see if could get meaningful clusters
```{r plot_clara4, echo = FALSE, fig.align="center", fig.cap="CLARA Approach. Two Feature, 4 Cluster Silhouette" , out.width = "70%"}

stats = victimStats[c("n_victims","place_type")] 
clara = clara(stats, k = 4, metric = "euclidean",stand = T, samples = 100, sampsize =500)
fviz_silhouette(clara, title = "") +
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())
```

Unfortunately the result is not great. Average silhouette coefficient is about 0.5; one cluster has a negative value. We have made quite a few experiments with the various number of clusters and metrics without much success (the results of the experiments are not published in the report). Some sources suggested to use dummy encoding for the categorical values. We tried - this approach did not help either. It looks like the categorical features are not very well suited for clustering analysis (this is what **H** stats actually hinted). 

Let's do another attempt this time we combine the number of victims and the number of gun used
```{r plot_clara5, fig.cap="CLARA Approach. Two Continuous Feature, 10 Cluster Silhouette", out.width = "70%"}
victimStats = data %>% filter(n_victims >= 3 & n_guns_involved>0) %>% arrange(date)
stats =  victimStats %>% select(n_victims, n_guns_involved)

clara = clara(stats, k = 10, metric = "euclidean",stand = T, samples = 100, sampsize =500)
fviz_silhouette(clara, title = "") +
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())
```

We have played with the number of clusters and found the one, which produces the most optimal result. Ten cluster give quite balanced split. The only exception is cluster # 4 which seems to contained misplaced observations. But if we employ different metrics would it help? Let us see. We use *manhattan* this time.
```{r plot_clara6,  fig.cap="CLARA Approach. Two Continuous Feature, 10 Cluster Silhouette. Manhattan Metrics", out.width = "70%"}

clara = clara(stats, k = 10, metric = "manhattan",stand = T, samples = 100, sampsize =500)
fviz_silhouette(clara, title = "") 
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())
```
Voilà! It worked. 

```{r plot_clara7, fig.cap="CLARA Approach. Two Continuous Feature, 10 Cluster Plot. Manhattan Metrics", out.width = "70%"}
fviz_cluster(clara, stats, stand = F,  geom = "point", main="",
   axes = c(1,2), xlab = "N of Victims", ylab = "Number of Guns") 

```

Geo-location of the clusters.
```{r plot_clara8,  fig.cap="CLARA Approach. Two Continuous Feature, 10 Clusters. Manhattan Metrics. Number of Victims and Guns Clusters on Map", out.width = "80%"}
combined = cbind(victimStats, cluster=c(clara$clustering))
ggplot(victimStats, aes(x=longitude, y=latitude )) + 
  geom_point( color = clara$clustering+2L)
```

So how do we interpret the clusters?

```{r plot_clara9, fig.cap="CLARA Approach. Number of Victims and Guns Cluster 3D Plot", echo = FALSE}

scatter3D( x = combined$cluster, y = combined$n_guns_involved, 
        z = combined$n_injured, 
        xlab = "Cluster", ylab = "Num of Guns", zlab = "Num of Injured",
        phi = 20, bty = "g", type="h", theta = 32,
        pch = 20, cex = 1.5, ticktype = "detailed",
        labels = combined$cluster, col = mainPalette)

```
Figure \ref{fig:plot_clara9} shows that the hideous crimes that number dozens of the injured victims are committed with greater number of the guns (cluster # 10). Another extreme is when a relatively low number of people is hurt but the number of guns found in the crime scene is extremely high. This could be attributed to a police raid.   

What if try to cluster dataset that includes geo-coordinates and categorical feature? Let's select observations that have number of victims killed greater or equal to 1, number of guns involved greater than zero and known place type.
```{r plot_clara10, fig.cap="CLARA Approach. Continuous and Categorical Feature, 5 Cluster Silhouette", out.width = "70%"}
victimStats = data %>% filter(n_victim_killed >= 1 & n_guns_involved>0 & place_type >0) %>% arrange(date)
stats =  victimStats %>% select(longitude, latitude, n_victims, place_type)

clara = clara(stats, k = 5, metric = "euclidean",stand = T, samples = 100, sampsize =500)
fviz_silhouette(clara, title = "") +
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())
```

Again the parallel clustering method does not yield the satisfactory result.

## Agglomerative Hierarchical Clustering Approach (AGNES). 

Next we are going to examine the same feature combinations using agglomerative clustering. We decided to choose the agglomerative clustering (AGNES) vs divisive method because the former generally has less challenges than the latter. In the case of the divisive method it is not always clear how to partition a large cluster into a smaller one (Ref: \cite{mining}). The agglomerative method starts from the bottom and increases the cluster size based on the selected metrics.

As in the case of *CLARA* we start with the univariate feature set, employing *euclidean* metrics, cutting the tree at 4 clusters


```{r plot_anges_s1,  fig.cap="AGNES Approach. 4 Cluster, Univariate Feature Silhouette", echo = FALSE, out.width = "70%"}
victimStats = data %>% filter(n_victims >= 3 & place_type>0) %>% arrange(date)
stats =  victimStats %>% select(n_victims)

agnes = factoextra::hcut(stats, k = 4,  hc_func ="agnes", 
       hc_method = "ward.D2", hc_metric = "euclidean",stand = T)

fviz_silhouette(agnes, title = "") +
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())
```
Following our routine we review the silhouette coefficients. They are less satisfactory than the ones produced by *CLARA*. Further experimentation with the number of the clusters and distance metrics did not produce better result. So we are moving on to the next feature set: *n_victims* and *n_guns_involved*

```{r plot_anges_s2, fig.cap="AGNES Approach. 4 Cluster, Two Feature Silhouette", echo = FALSE, out.width = "70%"}
victimStats = data %>% filter(n_victims >= 3 & n_guns_involved>0) %>% arrange(date)
stats =  victimStats %>% select(n_victims, n_guns_involved)

agnes = factoextra::hcut(stats, k = 4,  hc_func ="agnes", 
       hc_method = "ward.D2", hc_metric = "euclidean",stand = T)

fviz_silhouette(agnes, title = "") +
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())
```

Just like the previous attempt *AGNES* failed to provide any meaningful clusters.


## Density-based Clustering Approach (DBSCAN)

Unlike the hierarchical and parallel methods the density-based approach can deal with the clustered data of any shape. The density-based method do not require specification of number of clusters either. It is quite interesting to see if *DBSCAN* is able to discover data patterns overlooked by the previous two approaches (Ref: \cite{dbscan}) . In the case of **DBSCAN** we need at least two-dimensional data. Thus we will again be using *n_victims* and *n_guns_involved*

Prior to applying the algorithm we have to find the hyper parameters of the *DBSCAN*, namely: 

* the minimum distance between the point (**eps**).
* the minimum number of points to form a dense region (**minPoints**).

For that we will employ *k nearest neighbors* method. The idea is to calculate, the average of the distances of every point to its k nearest neighbors. The value of k will be specified by us and corresponds to **minPoints**. We will start with *k* to 5  

```{r plot_dbscan1,  fig.cap="DBSCAN Approach. Finding Hyper-Parameters", echo = FALSE, out.width = "60%"}
victimStats = data %>% filter(n_victims >= 3 & n_guns_involved > 0) %>% arrange(date)
stats =  victimStats %>% select(n_victims, n_guns_involved) %>% scale()
dbscan::kNNdistplot(stats, k = 5) +
abline(h=1.5, col=mainPalette[1], lty =  2) 
```

The plot (Figure: \ref{fig:plot_dbscan1}) has quite sharp elbow. It appears that **eps** = 1.5 would be a good starting value for 5 minimum points.  Let's run dbscan method

```{r plot_dbscan2,  fig.cap="DBSCAN Approach. Clusters", echo = FALSE, out.width = "70%"}
dbs = dbscan::dbscan(stats, eps = 1.5, minPts = 5) 
fviz_cluster(dbs, stats, stand = F,  geom = "point", main = "",
             axes = c(1,2), xlab = "Number of Victims", ylab = "Number of Guns")
```

The result of the first attempt is not satisfactory. The method identified one cluster and a lot of outliers. Let add geo-coordinates. Spatial data should introduce more dissimilarity which, hopefully, will be picked by the algorithm

```{r plot_dbscan3,  fig.cap="DBSCAN Approach. Finding Hyper-Parameters for High-dimentional Dataset", echo = FALSE, out.width = "60%"}
stats =  victimStats %>% select(longitude, latitude,n_victims) %>% scale()
dbscan::kNNdistplot(stats, k = 5) +
abline(h=1, col=mainPalette[1], lty =  2)
```

As per Figure \ref{fig:plot_dbscan3} we choose **1** for *eps*

```{r plot_dbscan4,  fig.cap="DBSCAN Approach. Clusters", echo = FALSE, out.width = "70%"}
dbs = dbscan::dbscan(stats, eps = 1, minPts = 5) 
fviz_cluster(dbs, stats, stand = F,  geom = "point", main = "",
             axes = c(1,2), xlab = "Longitude", ylab = "Latitude")
```

This approach has identified four clusters. Let's view how the incidents are spread geographically

```{r plot_dbscan5, fig.cap="DBSCAN Approach. Victims of Gun Violence on the Map", echo = FALSE, out.width = "90%"}
ggplot(victimStats, aes(x=longitude, y=latitude )) + 
  geom_point( color = dbs$cluster+1L)
```

Lastly we are going to increase dimentionality of the dataset adding **place_type** categorical feature. So we will be clustering data set that have geo-coordinates, number of victims and a place type.
```{r plot_dbscan6,  fig.cap="DBSCAN Approach. Finding Hyper-Parameters for High-dimentional Dataset with Categorical Feature", echo = FALSE, out.width = "60%"}
victimStats = data %>% filter(n_victims >= 3 & place_type > 0) %>% arrange(date)
stats =  victimStats %>% select(longitude, latitude,n_victims,place_type) %>% scale()
dbscan::kNNdistplot(stats, k = 5) +
abline(h=1.1, col=mainPalette[1], lty =  2)
```
We will be using *eps* = 1.1

```{r plot_dbscan7,  fig.cap="DBSCAN Approach. Clusters with Categorical features", echo = FALSE, out.width = "70%"}
dbs = dbscan::dbscan(stats, eps = 1.1, minPts = 5) 
fviz_cluster(dbs, stats, stand = F,  geom = "point", main = "",
             axes = c(1,2), xlab = "Number of Victims", ylab = "Number of Guns")
```

Let's review the cluster content.

```{r plot_dbscan8, fig.cap="DBSCAN Approach. Number of Victims and lace Type Cluster 3D Plot", echo = FALSE, out.width = "80%"}
combined = cbind(victimStats, cluster=c(dbs$cluster))
scatter3D( x = combined$cluster, y = combined$place_type, 
        z = combined$n_victims, 
        xlab = "Cluster", ylab = "Place Type", zlab = "Number of Vicitms",
        phi = 20, bty = "g", type="h", theta = 32,
        pch = 20, cex = 1.5, ticktype = "detailed",
        labels = combined$cluster, col = mainPalette)

```

As per Figure \ref{fig:plot_dbscan8} the latest **DBSCAN* run grouped the data geographically. It did not find *place type* and *number of victims* significant enough to form clusters based on those two features.   


## Clustering Method Evaluation 

We have applied three different clustering algorithm. The parallel clustering method **CLARA** performed the best on both univariate and multi-dimensional datasets. Tweaking the number of clusters and distance metrics we were able to find meaningful clusters with good separation and density. (see Figure \ref{fig:plot_clara1} and \ref{fig:plot_clara6} ). This approach did fail to cluster categorical data.

Neither hierarchical nor density-based approach performed particularly well. *DBSCAN* produced slightly better result when we added geo-coordinates, but the result was pretty meaningless. The density-based approach grouped data by longitude and latitude drawing the map of the USA...


# Model Deployment

Unfortunately we can state that the clustering methods were not effective for the selected dataset. Apparently hat frequency based analysis would yield much better result.


# Conclusion

We selected **Gun Violence in the US** dataset hoping to discover new relationship between various attributes, which would provide new insights into the goring problem of gun violence in the states. 

We spent significant efforts parsing and cleaning the data. We started with imputing missing geographic coordinates employing *OpenCage* geocoding service. Then we separated redundant and useful features. Majority of the useful features contained delimiter-separated, free-text values. We figured out the pattern and came up with the parsing method. We parsed the free-text data and added many new categorical.

We also processed descriptive features applying data mining techniques. We counted the most frequently used terms to understand the content of the features. We counted the words, bi-grams and tri-grams. we successfully identified the most common words and phrase and used them to add even more categorical features, enriching the dataset with meaningful data.

When the data preprocessing was done we measured Hopkins statistics to evaluate cluster tendency of the data set. The result was satisfactory; we proceeded with the clusterization. 

We applied various clustering approaches trying to find hidden trends and patterns. During this exercise we realized that the clustering methods did not work as well as we expected. We used *Sighloutte* test to validate the quality of the clusters. **CLARA** approach was the most successful among the three methods we picked (they were *CLARA*, *AGNES* and *DBSCAN*). We tried univariate and multivariate datasets, different metrics and number of clusters. Conducting our research we concluded that categorical features do not cluster well. We tried scaled features and dummy-encoded ones with the same less-than-satisfactory result.

Overall we were not able apply unsupervised learning to reach our goal, but we did learn about clustering approach and developed intuition in which situation it is the best to employ them.


\bibliography{RJreferences}


# Note from the Authors

This file was generated using [_The R Journal_ style article template](https://github.com/rstudio/rticles), additional information on how to prepare articles for submission is here - [Instructions for Authors](https://journal.r-project.org/share/author-guide.pdf). The article itself is an executable R Markdown file that could be [downloaded from Github](https://github.com/ivbsoftware/big-data-final-2/blob/master/docs/R_Journal/big-data-final-2/) with all the necessary artifacts.
