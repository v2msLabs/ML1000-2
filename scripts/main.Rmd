---
title: Gun Violence in the US. Application of Unsupervised Learning Methods for Trend Exploration
author:
  - name: Sumaira Afzal
    affiliation: York University School of Continuing Studies
  - name: Viraja Ketkar
    affiliation: York University School of Continuing Studies
  - name: Murlidhar Loka
    affiliation: York University School of Continuing Studies
  - name: Vadim Spirkov
    affiliation: York University School of Continuing Studies
abstract: >
  To do

output:
  rticles::rjournal_article:
    includes:
      in_header: preamble.tex
   
---

```{r echo=FALSE, message=FALSE, warnings=FALSE}
# Clean all variables that might be left by other script to avoid collusion
rm(list=ls(all=TRUE))
# load required libraries
library(ggplot2) # plotting lib
library(dplyr)  # data manipuation
library(RColorBrewer) # color palettes
library(xtable) # tabular data formatting 
library(tm) # text mining
library(summarytools) # nicer presentation of the dataframe summary
library(wordcloud) # plots fequent terms
library(factoextra) # deals with cluster plotting. Provides cluster related utility methods 
#library(cluster) # for gower similarity and pam
source('utils.R') # supplementary code



# set xtable properties for the project
options(xtable.floating = F)
options(xtable.timestamp = "")
options(xtable.comment = F)

# set summarytools global parameters
st_options(plain.ascii = F,       # This is very handy in all Rmd documents
      style = "rmarkdown",        # This too
      footnote = NA,             # Avoids footnotes which would clutter the result
      subtitle.emphasis = F,  # This is a setting to experiment with - according to
      dfSummary.graph.col = F
)  

# pick palettes
mainPalette = brewer.pal(8,"Dark2")
```

```{r global_options, include=FALSE}
# make the images flow nicely
knitr::opts_chunk$set(fig.pos = 'H', echo = T,comment = NA, prompt = F, 
                      cache = F, warning = F, message = F)
```


## Background


## Objective

The objective of this research is to ...


# Data Analysis

The data set used for this research contains 260k of gun violence incidents in the US between January 2013 and March 2018. The data has been sourced from [Kaggle](https://www.kaggle.com/jameslko/gun-violence-data). 

Originally the data set was uploaded to Kaggle from Gun Violence Archive (GVA)  Web site [ gunviolencearchive.org](https://www.gunviolencearchive.org/). This is a not for profit corporation formed in 2013 to provide free online public access to accurate information about gun-related violence in the United States. GVA will collect and check for accuracy, comprehensive information about gun-related violence in the U.S. and then post and disseminate it online. 


## Data Dictionary


Column Name                 | Column Description  
----------------------------| ------------------- 
incident_id                 | Incident ID
date                        | Date of crime
state                       | State
city_or_county              | City/county of crime
address                     | Address of the location of the crime
n_killed                    | Number of people killed
n_injured                   | Number of people injured
incident_url                | URL regarding the incident
source_url                  | Reference to the reporting source
incident_url_fields_missing | TRUE if the incident_url is present, FALSE otherwise
congressional_district      | Congressional district id
gun_stolen                  | Status of guns involved in the crime (i.e. Unknown, Stolen, etc...)
gun_type                    | Typification of guns used in the crime
incident_characteristics    | Characteristics of the incidence
latitude                    | Location of the incident
location_description        | Description of the location
longitude                   | Location of the incident
n_guns_involved             | Number of guns involved in incident
notes                       | Additional information of the crime
participant_age             | Age of participant(s) at the time of crime (victims and suspects)
participant_age_group       | Age group of participant(s) at the time crime
participant_gender          | Gender of participant(s)
participant_name            | Name of participant(s) involved in crime
participant_relationship    | Relationship of participant to other participant(s)
participant_status          | Extent of harm done to the participant
participant_type            | Type of participant (victim or suspect)
sources                     | Participants source
state_house_district        | Voting house district
state_senate_district       | Territorial district from which a senator to a state legislature is elected.


## Data Exploration

Firstly we are going to load and examine content and statistics of the data set

```{r}
data = read.csv("../data/gun-violence-data_01-2013_03-2018.csv", header = T, 
                na.strings = c("NA","","#NA"),sep=",")
```

```{r dataset_summary, echo=FALSE, results="asis"}
 print(dfSummary(data, valid.col = F, max.distinct.values = 4, heading = F),
       caption = "\\label{tab:dataset_summary} Gun Violence Dataset Summary")
```

Initial observation of the data shows that there is a number of features which do not present
any analytical value (Table:  \ref{tab:dataset_summary}). They are:

* *incident_id*
* *incident_url*
* *source_url* 
* *state_house_district*
* *state_senate_district* 
* *congressional_district*
* *sources*
* *incident_url_fields_missing*

We also going to drop *participant_age* feature in favor of the *participant_age_group*. The age group is more suitable for categorization and has much less missing data (16% vs 39%).   

The remaining features could be grouped as follows...

##### Participant Features.

This group describes suspects and victims found on the crime scene. The content of the features of this group is structured as follows: *[idx1::value1||idx2::value2]* (see Table: \ref{tab:dataset_summary}). This is not quite acceptable for the analytics, thus the participant related features would have to be parsed to extract valuable information about the crime.

It is feasible. *utils.R* script contains *parseFeature* function, which parses *[idx1::value1||idx2::value2]* structure and returns a named vector object. For example a  *participent_type* could be structured as follows:

0            | 1            | 2               | 3
-------------| -------------| ----------------| ---------------  
Victim       | Victim       | Subject-Suspect | Subject-Suspect

Unfortunately *participant_relationship* feature missing **93%** of values. It is not possible to impute the missing data thus we will drop it. For obvious reasons we are also going to get rid of *participant_name*. The rest of the participant-related features will be parsed and replaced wit the new categorical attributes. In order to do so we have to understand what possible values each participant-related feature can have. for this we will employ text mining technique.

We begin with *participant_type* feature

```{r}
participantType = data %>%  mutate(text = trimws(gsub('\\|\\||:|\\|'," ",participant_type, 
        fixed = F))) %>% filter(text != "0" ) %>% select(text)

pCorups = VCorpus(VectorSource(participantType))
pCorups  = tm_map(pCorups , removeNumbers)
pTermMatrix = tm::TermDocumentMatrix(pCorups)
# count frequent words
print(tm::findFreqTerms(pTermMatrix, 10))
```
As we can see the *participan type* may have two values *vitim* and *subject-suspect*. If the *participant type* is missing we will consider it as **unknown**. Thus we will be employing *participant type* feature as a basis to
impute all other participant stats.

Let's find the possible values of *parctipant_age_group* feature (the coded is omitted).
```{r echo=FALSE}
participantAgeGroup = data %>%  mutate(text = trimws(gsub('\\|\\||:|\\|'," ",participant_age_group,
      fixed = F))) %>% filter(text != "0" ) %>% select(text)
participantAgeGroup = distinct(participantAgeGroup)
pCorups = VCorpus(VectorSource(participantAgeGroup))
pCorups  = tm_map(pCorups , removeNumbers)
pTermMatrix = tm::TermDocumentMatrix(pCorups)
print(tm::findFreqTerms(pTermMatrix, 10))
```
Further examination of the feature data shows that there the age group values are:

* Adult 18+
* Teen 12-17
* Child 0-11

Thus using *participant_age_group* feature data we will create two new ones: *vicitm_age_group* and *suspect_age_group*. These new categorical features will be coded as follows: 

* 0 - no info
* 1 - all adults
* 2 - children/ teens
* 3 - adults and children/ teens . Adults make majority
* 4 - adults and children/ teens. Children/ teens make majority

*participant_gender* could also be parsed and replaced with the coded categorical features as described below.
```{r}
participantGender = data %>%  mutate(text = trimws(gsub('\\|\\||:|\\|'," ",participant_gender, 
        fixed = F))) %>% filter(text != "0" ) %>% select(text)
pCorups = VCorpus(VectorSource(participantGender))
pCorups  = tm_map(pCorups , removeNumbers)
pTermMatrix = tm::TermDocumentMatrix(pCorups)
print(tm::findFreqTerms(pTermMatrix, 10))
```
As a result we will be adding two new features:

* *victim_gender* - gender of the victims
* *suspect_gender* - gender of the suspects

**Gender Codes**

* 0 - no info
* 1 - male
* 2 - female
* 3 - male dominated group
* 4 - female dominated group

The last feature of the group is *participant_status*. It maintains the outcome of the incident. Let's review the content of the attribute.

```{r echo=FALSE}
participantStatus = data %>%  mutate(text = trimws(gsub('\\|\\||:|\\|'," ",participant_status, 
        fixed = F))) %>% filter(text != "0" ) %>% select(text)
pCorups = VCorpus(VectorSource(participantStatus))
pCorups  = tm_map(pCorups , removeNumbers)
pTermMatrix = tm::TermDocumentMatrix(pCorups)
print(tm::findFreqTerms(pTermMatrix, 10))
```
Based on our findings we will be creating three new numerical features:

* n_victim_killed - number of victims killed
* n_victim_injured - number of victims injured
* n_arrested - number of suspects arrested

##### Gun Related Features. 

There are three attributes that describe gun types: *gun_stolen*, *gun_type* and *n_guns_invoved* (Table: \ref{tab:dataset_summary}) *gun_type* and *gun_stolen* have similar to the participant-related features encoding (*[idx1::value1||idx2::value2]*). Thus they also could be parsed and substituted with the categorical features.
We begin with the gun type.
```{r plot_gun_types,  echo=FALSE, fig.align="center", fig.cap="The Most Frequently Used Gun Types"}
gunTypes = data %>%  mutate(text = trimws(gsub('\\|\\||:|\\||Unknown'," ",gun_type, fixed = F))) %>% 
  filter(text != "0" ) %>% select(text)
pCorups = VCorpus(VectorSource(gunTypes))
pCorups  = tm_map(pCorups , removeNumbers)
pCorups  = tm_map(pCorups , removePunctuation)
pTermMatrix = tm::TermDocumentMatrix(pCorups)
tmp = as.matrix(pTermMatrix)
tmp = sort(rowSums(tmp),decreasing=T)
tmp = data.frame(word = names(tmp),freq=tmp)
set.seed(1234)
wordcloud(words = tmp$word, freq = tmp$freq, min.freq = 1,max.words=100, random.order=F,
          rot.per=0.35, colors=mainPalette)
```
Employing simple text mining techniques we can see that **handgun**, **rifle**, **shotgun** and **auto** make the majority. Thus we will add another new feature *gun_type_involved* to categorize the gun types as follows:

* 0 - unknown 
* 1 - handgun
* 2 - shotgun/ rifle
* 3 - automatic
* 4 - mix/other 

*gun_stolen* attribute tells if the gun was stolen or acquired legally. We are going to create a new categorical feature - *gun_origin* which would maintain the following data:

* 0 - unknown
* 1 - all stolen
* 2 - all acquired legally
* 3 - mix of stolen and legal guns


##### Location Related Features.

To analyze geography of the crimes we will be employing *state*, *city_or_county*, *latitude* and *longitude* attribute. since we have the coordinates the *address* feature does not present much value for unsupervised learning. We will be using it though to impute missing latitude and longitude values. This activity will be covered in greater details in **Missing Data** paragraph.

#### Descriptive Features.

*notes*, *location_description* and *incident_characteristics* are free-text features that might provide additional insights about the crime scene. We are going to take a close look at each feature and decide if we could utilize it.

Lets' begin with the *notes*
```{r plot_notes,  echo=FALSE, fig.align="center", fig.cap="Most Common Words in Notes"}
notes = data %>%  mutate(text = trimws(notes)) %>% filter(!is.na(text)) %>% select(text)
pCorups = VCorpus(VectorSource(notes))
pCorups  = tm_map(pCorups , removeNumbers)
pCorups  = tm_map(pCorups , removePunctuation)
pTermMatrix = tm::TermDocumentMatrix(pCorups)
tmp = as.matrix(pTermMatrix)
tmp = sort(rowSums(tmp),decreasing=T)
tmp = data.frame(word = names(tmp),freq=tmp)
set.seed(5673)
wordcloud(words = tmp$word, freq = tmp$freq, min.freq = 10,max.words=200, random.order=F,
          rot.per=0.35, colors=mainPalette)
```
Unfortunately *notes* feature does not provide more knowledge to what the others features already supply. Thus it will be dropped.

*location_description* on the other hand, could be useful to classify location type. Unfortunately 82% of the data is missing. Nonetheless this feature appears to be too important to ignore. Let's see how much we can salvage.
```{r plot_location,  echo=FALSE, fig.align="center", fig.cap="Most Common Bi-gram Terms in Location Decription"}
location = data %>%  mutate(text = trimws(location_description)) %>% filter(!is.na(text)) %>% select(text)
pCorups = VCorpus(VectorSource(location))
pCorups  = tm_map(pCorups , removeNumbers)
pCorups  = tm_map(pCorups , removePunctuation)
pCorups = tm_map(pCorups, removeWords, stopwords())
pTermMatrix = tm::TermDocumentMatrix(pCorups, control = list(tokenize = BigramTokenizer))
tmp = as.matrix(pTermMatrix)
tmp = sort(rowSums(tmp),decreasing=T)
tmp = data.frame(word = names(tmp),freq=tmp)
set.seed(901)
wordcloud(words = tmp$word, freq = tmp$freq,min.freq=30, max.words=100, random.order=F,
          rot.per=0.3, colors=mainPalette,scale=c(3,.5))
```

As plot \ref{fig:plot_location} shows we could utilize bi-gram terms if the location description is available. Thus let's add new categorical feature - *place_type*, which would have the following values:

* 0 - unknown
* 1 - school/ university/ college
* 2 - community center/ shopping center/ hospital/ church
* 3 - home invasion
* 4 - street/drive by
* 5 - other public places

The last feature in the group is *incident_characteristics*. The feature is almost 100% populated. As with the previous two we are going to find out what info it maintains.

```{r include=FALSE}
characteristics = data %>%  mutate(text = gsub('\\|\\||/'," ",incident_characteristics, fixed = F))  %>%                 filter(!is.na(text)) %>% select(text)
pCorups = VCorpus(VectorSource(characteristics))
pCorups  = tm_map(pCorups , removeNumbers)
pCorups  = tm_map(pCorups , removePunctuation)
pCorups = tm_map(pCorups, removeWords, stopwords())
pTermMatrix = tm::TermDocumentMatrix(pCorups, control = list(tokenize = BigramTokenizer))
tmp = as.matrix(pTermMatrix)
tmp = sort(rowSums(tmp),decreasing=T)
tmp = data.frame(word = names(tmp),freq=tmp)

```
```{r echo=TRUE, results="asis"}
print(xtable(tmp[1:50,1:2],
      caption = "\\tt Most Frequently Used Incident Characteristics  Bi-grams"),
      row.names = FALSE,  include.rownames = FALSE)
```

Information the *incident_characteristics* provides proved to be useful. It can support two features: *place_type*, which was introduced above and *inceident_type*. The *incident_type* is going to be a categorical attribute with the following codes:

* 0 - unknown
* 1 - accidental
* 2 - defensive use
* 3 - armed robbery
* 4 - suicide
* 5 - raid/ arrest/ warrant
* 6 - domestic violence
* 7 - gun brandishing, flourishing, open demonstration 
_
We are also be adding a feature that indicates if drugs or alcohol was involved: *is_drug_alcohol*

##### Date Feature

In addition to *date* of incident attribute we add *month*  and *day_of_week* to identify any seasonal patterns. 


## Data Preparation

Prior to generating new features as discussed in the previous paragraph we would need to impute missing latitude and longitude data. To do so we employ [OpenCage](https://opencagedata.com/) forward geocoding API. Unlike Google this company offers a free tier. To save time we imputed the missing geo-coordinates and saved the result in the file. The code below is submitted for demonstration purpose only.
```{r, eval = FALSE, echo = TRUE}
  imputeCoordinates()
```

Now we are going to remove the features identified as redundant
```{r}
data = subset(data, select = c(-incident_id, -incident_url, -source_url, 
  -state_house_district, -state_senate_district,-sources, -incident_url_fields_missing,
  -congressional_district, -address, -participant_age, -participant_name,
  -participant_relationship, -notes))
```

Lastly we are going to loop through the entire data fame imputing missing data and adding new features. Again this is a lengthy process that takes about 1.5 hours to finish. The code is also quite long. Thus in order not to clutter the report we submit the code just in the script to illustrates the process, but will not output it into the report.

```{r eval=FALSE, include=FALSE}
for (idx in 1:nrow(data)) {
 date = data[idx,"date"]
 data[idx,"month"] = as.numeric( format(date, "%m")) 
 data[idx,"day_of_week"] = as.numeric( format(date, "%u")) 
 # parse participant gender  
 participantGender = parseFeature(data[idx,"participant_gender"])  
 # parse participant type
 participantType = parseFeature(data[idx,"participant_type"])  
 # parse participant age group
 participantAgeGroup = parseFeature(data[idx,"participant_age_group"])  
 # parse participant status
 participantStatus = parseFeature(data[idx,"participant_status"])  
 # parse gun origin
 gunOrigin = data[idx,"gun_stolen"]  
 # parse gun type
 gunType = data[idx,"gun_type"]  
 # assign other features of interest to variables to optimize performance and simplify the reference
 numKilled = data[idx,"n_killed"]
 numInjured = data[idx,"n_injured"]
 numGuns = data[idx,"n_guns_involved"]
 characteristics = data[idx,"incident_characteristics"]
 locationDecription  = data[idx,"location_description"]

 # default new, participant-related  features
 data[idx,"victim_gender"]  = 0
 data[idx,"suspect_gender"]  = 0 
 data[idx,"victim_age_group"]  = 0 
 data[idx,"suspect_age_group"]  = 0 
 data[idx,"n_victim_killed"]  = 0 
 data[idx,"n_victim_injured"]  = 0 
 data[idx,"n_victims"]  = 0 
 data[idx,"n_suspects"]  = 0
 data[idx,"n_arrested"] = 0
 # process participant-related data
 if (!all(is.na(participantType))) {
   victims = participantType[which(participantType %in% c("Victim"))]
   suspects = participantType[which(participantType %in% c("Subject-Suspect"))]
   
   n_victims = length(victims)
   n_suspects = length(suspects)
   data[idx,"n_victims"] = n_victims
   data[idx,"n_suspects"] = n_suspects

   if (n_victims > 0) {
     
     injured = 0
     killed = 0
     female = 0
     male = 0
     child = 0
     teen = 0
     adult = 0
     
     for (i in names(victims)){
       ## outcome
       status = participantStatus[i]
       if ( !isNaOrNull(status) ) {
         if (str_count(status,fixed("injured", ignore_case = T)) > 0) {
           injured = injured + 1
         } else if (str_count(status,fixed("killed", ignore_case = T)) > 0) {
           killed = killed + 1
         } 
       }   
       # gender
       gen = participantGender[i]
       if (!isNaOrNull(gen)) {
         if (str_count(gen,regex("^male", ignore_case = T)) > 0) {
           male = male + 1
         } else if (str_count(gen,regex("^female", ignore_case = T)) > 0) {
           female = female + 1
         }
       }
       # age group
       age = participantAgeGroup[i]
       if (!isNaOrNull(age)) {
         if (str_count(age,fixed("teen", ignore_case = T)) > 0) {
           teen = teen + 1
         } else if (str_count(age,fixed("adult", ignore_case = T)) > 0) {
           adult = adult + 1
         } else if (str_count(age,fixed("child", ignore_case = T)) > 0) {
           child = child + 1
         } 
       }
     }
     
     # summarize
     data[idx,"n_victim_killed"] = killed
     data[idx,"n_victim_injured"] = injured
     if ((child > 0|| teen > 0) && adult  == 0) {
       data[idx,"victim_age_group"] = 2
     } else if ( (child > 0|| teen > 0) && adult >0){
       data[idx,"victim_age_group"] = ifelse(child + teen >= adult, 4, 3)
     } else if (adult > 0) {
       data[idx,"victim_age_group"] = 1   
     }
     
     
     if (female > 0 && male > 0) {
       data[idx,"victim_gender"] = ifelse(female > male, 4, 3)
     } else if (male > 0) {
       data[idx,"victim_gender"] = 1
     } else if (female > 0) {
       data[idx,"victim_gender"] = 2
     }
     
   }
 
   if (n_suspects >0) {
     arrested = 0
     female = 0
     male = 0
     child = 0
     teen = 0
     adult = 0  
     for (i in names(suspects)){
       ## outcome
       status = participantStatus[i]
       if ( !isNaOrNull(status) && str_count(status,fixed("arrested", ignore_case = T)) > 0) {
         arrested = arrested + 1
       }  
       # gender
       gen = participantGender[i]
       if (!isNaOrNull(gen)) {
         if (str_count(gen,regex("^male", ignore_case = T)) > 0) {
           male = male + 1
         } else if (str_count(gen,regex("^female", ignore_case = T)) > 0) {
           female = female + 1
         }
       }
       # age group
       age = participantAgeGroup[i]
       if (!isNaOrNull(age)) {
         if (str_count(age,fixed("teen", ignore_case = T)) > 0) {
           teen = teen + 1
         } else if (str_count(age,fixed("adult", ignore_case = T)) > 0) {
           adult = adult + 1
         } else if (str_count(age,fixed("child", ignore_case = T)) > 0) {
           child = child + 1
         } 
       }
     }
     
     # summarize
     data[idx,"n_arrested"] = arrested
     
     if ((child > 0|| teen > 0) && adult  == 0) {
       data[idx,"suspect_age_group"] = 2
     } else if ( (child > 0|| teen > 0) && adult >0){
       data[idx,"suspect_age_group"] = ifelse(child + teen >= adult, 4, 3)
     } else if (adult > 0) {
       data[idx,"suspect_age_group"] = 1   
     }
     
     if (female > 0 && male >0) {
       data[idx,"suspect_gender"] = ifelse(female > male, 4, 3)
     } else if (male > 0) {
       data[idx,"suspect_gender"] = 1
     } else if (female > 0) {
       data[idx,"suspect_gender"] = 2
     }   
   }
 }
 
 if ( isNaOrNull(numKilled) ) {
   data[idx,"n_killed"] = ifelse(data[idx,"n_victim_killed"] > 0,data[idx,"n_victim_killed"],0 )
 }
 
 if ( isNaOrNull(numInjured) ) {
   data[idx,"n_injured"] = ifelse(data[idx,"n_victim_injured"] > 0,data[idx,"n_victim_injured"],0 )
 }
 
 
 # process gun-related data
 data[idx,"gun_type_involved"]  = 0 
 data[idx,"gun_origin"]  = 0

  # gun origin
 if ( !isNaOrNull(gunOrigin) ) {
   stolen = str_count(gunOrigin,fixed("stolen", ignore_case = T))
   notStolen = str_count(gunOrigin,fixed("notstolen", ignore_case = T))
   if (stolen > 0 && notStolen > 0) {
     data[idx,"gun_origin"] = 3
   } else if (stolen > 0){
     data[idx,"gun_origin"] = 1
   } else if (notStolen > 0){
     data[idx,"gun_origin"] = 2
   }
 }

 shotgun = 0
 handgun = 0
 auto = 0
 other = 0
 if ( !isNaOrNull(gunType) ) { 
   shotgun = str_count(gunType,regex("shotgun|rifle", ignore_case = T))
   auto = str_count(gunType,regex("auto|ak-", ignore_case = T))
   handgun = str_count(gunType,regex("handgun|spl", ignore_case = T))
   other = str_count(gunType,regex("gauge|mag|other|rem|spr|win", ignore_case = T)) 
   
   if (other == 0 && shotgun == 0 && auto == 0 && handgun > 0 ) {
     data[idx,"gun_type_involved"]  = 1
   } else if (shotgun > 0 && auto == 0 && handgun == 0 && other == 0) {
     data[idx,"gun_type_involved"]  = 2
   } else if (shotgun == 0 && auto > 0 && handgun == 0 && other == 0) {
     data[idx,"gun_type_involved"]  = 3
   } else if (shotgun > 0 || auto > 0 || handgun> 0 || other > 0) {
     data[idx,"gun_type_involved"]  = 4
   } 
   
 }
 
 if ( isNaOrNull(numGuns) ) {
   data[idx,"n_guns_involved"] = shotgun + handgun + auto + other
 }
 
 # process location and characteristics 
 data[idx,"place_type"]  = 0 
 data[idx,"incident_type"]  = 0
 data[idx,"is_drug_alcohol"] = 0

 if ( !isNaOrNull(locationDecription) ) { 
   if (str_count( locationDecription,regex("school|university|college", ignore_case = T)) > 0){
     data[idx,"place_type"]  = 1
   } else if (str_count( locationDecription,regex("music hall|community|dollar|convenience store|circle k|church|food mart|medical center|shopping center|mall|hospital", ignore_case = T)) > 0) {
     data[idx,"place_type"]  = 2
   } else  if (str_count( locationDecription,regex("apartment|back yards|apartments|mobile home", ignore_case = T)) > 0) {
     data[idx,"place_type"]  = 3
   } else  if (str_count( locationDecription,regex("car", ignore_case = T)) > 0) {
     data[idx,"place_type"]  = 4
   }else  if (str_count( locationDecription,regex("village|neighborhood|bar|park|gas station|grill|east garfield|west garfield|waffle house|jail|taco bell|police department|bank|club|restaurant|pizza|inn|hotel|office", ignore_case = T)) > 0) {
     data[idx,"place_type"]  = 5
   }
 }

 if ( !isNaOrNull(characteristics) ) {
   if (str_count( characteristics,regex("accidental shooting|accidental suicide|murder accidental|injury accidental|accidental negligent|negligent discharge", ignore_case = T)) > 0){
     data[idx,"incident_type"]  = 1
   } else if (str_count( characteristics,regex("defensive use|use defensive|dgu", ignore_case = T)) > 0) {
     data[idx,"incident_type"]  = 2
   } else if (str_count( characteristics,regex("armed robbery|robbery with", ignore_case = T)) > 0) {
     data[idx,"incident_type"]  = 3
   } else if (str_count( characteristics,regex("suicide shot|suicide nonshooting|suicide officer", ignore_case = T)) > 0) {
     data[idx,"incident_type"]  = 4
   } else if (str_count( characteristics,regex("arrest|raid|warrant", ignore_case = T)) > 0) {
     data[idx,"incident_type"]  = 5
   } else if (str_count( characteristics,regex("domestic violence", ignore_case = T)) > 0) {
     data[idx,"incident_type"]  = 6
   } else if (str_count( characteristics,regex("brandishing|flourishing|open carry", ignore_case = T)) > 0) {
     data[idx,"incident_type"]  = 7
   }
   # impute place_type if required and possible
   if (data[idx,"place_type"] == 0) {
     if (str_count( characteristics,regex("home invasion", ignore_case = T)) > 0) {
       data[idx,"place_type"]  = 3
     } else if (str_count( characteristics,regex("car to car|car to street|drive-by|car shot", ignore_case = T)) > 0) {
       data[idx,"place_type"]  = 4
     } else if (str_count( characteristics,regex("business|institution group|club|bar", ignore_case = T)) > 0) {
       data[idx,"place_type"]  = 5
     } 
   }
   # if drugs/alcohol
   data[idx,"is_drug_alcohol"] = ifelse(str_count( characteristics,regex("drug|atf|alcohol|under influence",
                                  ignore_case = T)) > 0, 1, 0)   
     
 }   

}  
```

After we added new feature it is time to remove the columns that are no longer relevant and save the result into a file to be used for unsupervised learning

```{r}
data = subset(data, select = c(-participant_age_group, -participant_type,
      -participant_gender, -participant_status, -location_description, 
      -incident_characteristics, -gun_stolen,-gun_type))

```
```{r include=FALSE}

rm(data, gunTypes,location,notes, participantAgeGroup, participantGender,participantStatus,
   participantType, pCorups, pTermMatrix, tmp, characteristics)
```


## Resulting Dataset

After a rather lengthy process, we finaly have reached the stage when our data set is eady to be used for exploration by clustering algorithms. All missing features have been imputed and free-format text columns have been relaced with the categorical attributes.  This is the summary of the resulting data.


```{r include=FALSE}
data = read.csv("../data/gun-violence-engineered.csv", header = T,sep=",")
```
```{r dataset_engineered, results = 'asis'}
 print(dfSummary(data, valid.col = F, max.distinct.values = 4, headings = F),
       caption = "\\tt Engineered Gun Violence Dataset Summary")
```


# Modeling and Evalutation

In this section we will apply various clustering methods to explore gun violence trends in US. We will empoy parallel, hierarchical and density-based clustering approaches. Then we evaluate and compare the resluts produced by each approach. 

In general the clustering algorithms take time to compute the result. Thus we will be employing a smaller dataset to find and visualize the clusters. We will be exploring the incidents, that have at least three victims and a where a place type was recorded.

```{r}
victimStats = data %>% filter(n_victims >= 3 & place_type>0) %>% arrange(date)
victimStatsCont = victimStats[c("n_victims","n_suspects","n_guns_involved","n_killed","n_injured")] %>% scale()
```

Before we attempt to apply clustering models to the dataset we should assess clustering tendency. In order to do so we will employ **Hopkins** statistics.

## Hopkins Statistics

Hopkins statistic is used to assess the clustering tendency of a dataset by measuring the probability that a given dataset is generated by a uniform data distribution. Let's calculcate Hopkins (**H**) statistics for all  continuous variables:

* n_guns_involved
* n_victims
* n_suspects
* n_killed
* n_injured

The **H** value close to zero indicates very good clustering tendency. The **H** value around or greater than 0.5 denotes poor clustering tendency. We 

```{r}
H =  get_clust_tendency(victimStatsCont,n = 100, graph = F, seed = 6709)
print(H[["hopkins_stat"]])
```

Outsanding!  **H** value of **0.0131267** is very encouraging. Let' calculate Hopkins statistics for all the features but geo-coordinates.

```{r}
H =  get_clust_tendency(victimStats[,3:4], n = 100, graph = F, seed = 6701)
print(H[["hopkins_stat"]])
```

And again the result is very good. Now it is time to explore the data


## Partitioning Clustering Approach



## Agglomerative Hierarchical Clustering Approach (AGNES). 

We decided to choose agglomerative clustering vs devisive method because the former generally has less challanges than th latter. In the case of the devisive method it is not quite clear how to aprtition a large cluster into a smaller one. We opted to cut the tree at six clusters. We employed *euclidean* disimaliy 


So in conclusion, I believe that categorical data does not cluster in the way clustering is commonly defined because the discrete nature yields too little discrimination/ranking of similarities. It may have frequent patterns as detected e.g. by Apriori, but that is a very different definition. And how to combine these two is not obvious. So for categorical data, I recommend frequent patterns. These make much more sense than "clusters".

```{r}
# http://eric.univ-lyon2.fr/~ricco/cours/slides/en/classif_interpretation.pdf

victimStats1 = victimStats[c("","n_victims","place_type")] 
victimStats1S = victimStats1 %>% scale()

agnesCont = factoextra::hcut(victimStats1S, k = 6,  hc_func ="agnes", 
                         hc_method = "ward.D2", hc_metric = "euclidean",stand = F)

#victim2StatsCont = victimStats[c("n_victims","n_suspects","n_guns_involved","n_killed","n_inj#ured")]

fviz_silhouette(agnesCont)

fviz_cluster(agnesCont,   geom = "point",
             axes = c(1,2), xlab = "N Victims", ylab = "Place")

agnesData = cbind(victimStats1,cluster=agnesCont$cluster)

c1 = agnesData %>% filter(cluster == 1) 

plot(c1$place_type,c1$n_victims)

c2 = agnesData %>% filter(cluster == 2) 

plot(c2$place_type,c2$n_victims)

library(fastDummies)

victim3StatsCont =  mutate(victimStats, place_type = as.factor(place_type)) %>% 
  select(place_type, n_victims)

victim3StatsCont = fastDummies::dummy_cols(victim3StatsCont,  select_columns = "place_type")

victim3StatsCont = victim3StatsCont[,-1]
victim33StatsCont = victim3StatsCont %>% scale()


agnes3Cont = factoextra::hcut(victim3StatsCont, k = 10,  hc_func ="agnes", 
                         hc_method = "ward.D2", hc_metric = "euclidean",stand = F)

agnes3Data = cbind(victim3StatsCont,cluster=agnes3Cont$cluster)

fviz_cluster(agnes3Cont,   geom = "point",
             axes = c(1,2), xlab = "N Victims", ylab = "N Suspects")

fviz_silhouette(agnes3Cont)





victimStats2 = subset(victimStats, select = c(n_victims, n_suspects))
vs2  = victimStats2 %>% scale()
agnes = factoextra::hcut(victimStats2, k = 10,  hc_func ="agnes", 
                         hc_method = "ward.D2", hc_metric = "euclidean",stand = T)

agnes2 = factoextra::hcut(vs2, k = 6,  hc_func ="agnes", 
                         hc_method = "ward.D2", hc_metric = "euclidean",stand = F)
comAgnes =  cbind(agnes$cluster,agnes2$cluster)



agnesData = cbind(victimStats,agnes$cluster)

agnesData2 = cbind(victimStats2,agnes2$cluster)

victimStats3 = mutate(victimStats2, place_type = as.factor(place_type))


gower.dist <- daisy(victimStats3, metric = c("gower"))

agnes3 = factoextra::hcut(as.matrix(victimStats3), k = 10,  hc_func ="agnes", 
                         hc_method = "ward.D2", hc_metric = "euclidean",stand = F)
str(victimStats3)
agnesData3 = cbind(victimStats3,agnes3$cluster)
fviz_silhouette(agnes)
# Visualize clusters as scatter plots
fviz_cluster(agnes)

```


## Density-based Clustering Methods



## Clustering Method Evaluation



# Model Deployment


# Conclusion



\bibliography{RJreferences}


# Note from the Authors

This file was generated using [_The R Journal_ style article template](https://github.com/rstudio/rticles), additional information on how to prepare articles for submission is here - [Instructions for Authors](https://journal.r-project.org/share/author-guide.pdf). The article itself is an executable R Markdown file that could be [downloaded from Github](https://github.com/ivbsoftware/big-data-final-2/blob/master/docs/R_Journal/big-data-final-2/) with all the necessary artifacts.
